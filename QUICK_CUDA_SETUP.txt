QUICK SETUP - Sign2Sound with CUDA (8GB VRAM)
Repository and dataset already downloaded!

STEP 1: Verify CUDA
nvidia-smi && nvcc --version

STEP 2: Setup environment
cd SIGN2SOUND_ABH
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install --upgrade pip
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt
python -c "import torch; print('CUDA:', torch.cuda.is_available(), 'GPU:', torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None')"

STEP 3: Download MediaPipe model
wget -O models/hand_landmarker.task https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/latest/hand_landmarker.task

STEP 4: Preprocess data (~45 min)
python scripts/preprocess_asl_images.py --train-only --create-splits
python scripts/preprocess_asl_images.py --test-only

STEP 5: Train with CUDA (~2-3 hours)
export CUDA_VISIBLE_DEVICES=0
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
python training/train.py --config training/config.yaml --use-cuda

Monitor: watch -n 1 nvidia-smi

STEP 6: Evaluate
python training/evaluate.py --model checkpoints/best_model.h5 --use-cuda

STEP 7: Test real-time
python inference/realtime_demo.py --model checkpoints/best_model.h5 --use-cuda

STEP 8: Run UI
cd ui && python app.py

Expected: 90-95% accuracy, <15ms inference, 2-3 hours training
