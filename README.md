<div align="center">

# ğŸ—£ï¸ Sign2Sound

### *Bridging the gap between Sign Language and Spoken English with Real-Time, Edge-Computed AI*

[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![MediaPipe](https://img.shields.io/badge/MediaPipe-0.10+-00C4CC?style=for-the-badge&logo=google&logoColor=white)](https://mediapipe.dev/)
[![Qwen2.5](https://img.shields.io/badge/Qwen2.5-LLM-FF6B6B?style=for-the-badge&logo=ai&logoColor=white)](https://huggingface.co/Qwen)
[![KokoroTTS](https://img.shields.io/badge/KokoroTTS-Voice-9B59B6?style=for-the-badge&logo=speaker&logoColor=white)](https://github.com/hexgrad/kokoro)
[![License](https://img.shields.io/badge/License-MIT-blue?style=for-the-badge)](LICENSE)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ‘‹ Sign Language  â†’  ğŸ§  AI Processing  â†’  ğŸ”Š Natural Speech â”‚
â”‚                    Fully Offline & Private                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

[Features](#-key-innovations) â€¢ [Architecture](#ï¸-system-architecture) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage) â€¢ [Team](#-team)

</div>

---

## ğŸ“– Overview

**Sign2Sound** is a cutting-edge, real-time sign language translation system designed to run **entirely on consumer-grade hardware** with **zero cloud dependency**. 

ğŸ¯ **Mission**: Empower deaf and hard-of-hearing individuals with accessible, privacy-first communication technology.

### ğŸŒŸ What Makes Us Different?

<table>
<tr>
<td width="33%" align="center">
<h3>ğŸ”’ Privacy First</h3>
<b>100% Offline</b><br/>
Zero data leaves your device
</td>
<td width="33%" align="center">
<h3>âš¡ Edge-Optimized</h3>
<b>20+ FPS</b><br/>
Real-time on consumer laptops
</td>
<td width="33%" align="center">
<h3>ğŸ§  AI-Powered</h3>
<b>Qwen2.5 + KokoroTTS</b><br/>
Natural language & voice
</td>
</tr>
</table>

---

## ğŸš€ Key Innovations

<div align="center">

| Innovation | Description | Impact |
|:----------:|:------------|:------:|
| ğŸ¯ **Temporal Intelligence** | LSTM/Transformer models capture motion dynamics & sign boundaries | ğŸ”¥ High Accuracy |
| ğŸš„ **Edge Computing** | Optimized for resource-constrained devices | âš¡ Real-time |
| ğŸ¦´ **Landmark-Based** | MediaPipe skeletal extraction (interpretable & lightweight) | ğŸ“Š Explainable AI |
| ğŸ¤– **Qwen2.5 Grammar** | Small Language Model corrects glosses to natural English | ğŸ’¬ Natural Output |
| ğŸ”Š **KokoroTTS Voice** | High-fidelity, low-latency speech synthesis | ğŸµ Human-like |
| ğŸ” **Privacy-Centric** | No cloud APIs, no data collection | ğŸ›¡ï¸ Secure |

</div>

---

## ğŸ› ï¸ System Architecture

<div align="center">

```mermaid
graph LR
    A[ğŸ“¹ Video Input] --> B[ğŸ¦´ MediaPipe<br/>Skeletal Extraction]
    B --> C[âš™ï¸ Temporal Feature<br/>Engineering]
    C --> D[ğŸ§  LSTM/Transformer<br/>Sign Classification]
    D --> E[ğŸ¤– Qwen2.5 LLM<br/>Grammar Correction]
    E --> F[ğŸ”Š KokoroTTS<br/>Speech Synthesis]
    F --> G[ğŸµ Natural Speech Output]
    
    style A fill:#3498db,stroke:#2980b9,color:#fff
    style B fill:#e74c3c,stroke:#c0392b,color:#fff
    style C fill:#f39c12,stroke:#d68910,color:#fff
    style D fill:#9b59b6,stroke:#8e44ad,color:#fff
    style E fill:#FF6B6B,stroke:#e74c3c,color:#fff
    style F fill:#9B59B6,stroke:#8e44ad,color:#fff
    style G fill:#2ecc71,stroke:#27ae60,color:#fff
```

</div>

### ğŸ” Pipeline Stages

<details>
<summary><b>1ï¸âƒ£ Skeletal Extraction (MediaPipe Holistic)</b></summary>

- **Tool**: Google MediaPipe Holistic
- **Data**: Extracts hand landmarks (21 keypoints/hand) and pose landmarks (33 keypoints)
- **Normalization**: Wrist-relative or shoulder-relative alignment for pose invariance
- **Output**: 126-dimensional feature vectors per frame

</details>

<details>
<summary><b>2ï¸âƒ£ Temporal Feature Engineering</b></summary>

- **Sequences**: Sliding window approach to capture temporal context
- **Features**: Velocity, acceleration, and spatial relationships between landmarks
- **Augmentation**: Temporal jitter, spatial rotation, and scaling for robustness
- **Buffer**: Dynamic sequence buffering for variable-length signs

</details>

<details>
<summary><b>3ï¸âƒ£ Sign Classification (Temporal Model)</b></summary>

- **Architecture**: Bidirectional LSTM or lightweight Transformer encoder
- **Input**: Normalized landmark sequences (126-dimensional vectors per frame)
- **Output**: Raw glosses (e.g., "WHO EAT NOW")
- **Optimization**: Quantized for edge deployment (< 10MB)

</details>

<details>
<summary><b>4ï¸âƒ£ Grammar Correction (Qwen2.5 LLM) ğŸ¤–</b></summary>

> **ğŸŒŸ Powered by Qwen2.5 Small Language Model**

- **Model**: Quantized Qwen2.5-1.5B-Instruct (4-bit GPTQ)
- **Task**: Convert raw sign glosses into grammatically correct English
- **Example**: 
  - Input: `"WHO EAT NOW"`
  - Output: `"Who is eating now?"`
- **Latency**: < 200ms on CPU
- **Context-Aware**: Understands sign language grammar patterns

</details>

<details>
<summary><b>5ï¸âƒ£ Speech Synthesis (KokoroTTS) ğŸ”Š</b></summary>

> **ğŸµ Powered by KokoroTTS - High-Fidelity Neural TTS**

- **Engine**: KokoroTTS (Lightweight, Expressive Neural TTS)
- **Quality**: Natural, human-like voice with emotional intonation
- **Latency**: < 80ms for real-time synthesis
- **Offline**: Fully local execution, no API calls
- **Voices**: Multiple voice profiles available

</details>

---

## ğŸ“Š Performance Metrics

<div align="center">

### ğŸ¯ Benchmark Results

| Metric | Value | Hardware | Notes |
|:------:|:-----:|:--------:|:------|
| **ğŸ¯ Accuracy** | **85-90%** | Consumer Laptop | On operational sign set (30-50 signs) |
| **âš¡ Latency** | **< 100ms** | CPU (Intel i5) | Per sign inference |
| **ğŸ¬ FPS** | **20-25** | Webcam 720p | Real-time processing |
| **ğŸ’¾ Model Size** | **< 10MB** | Compressed | LSTM/Transformer model |
| **ğŸ¤– Grammar Latency** | **< 200ms** | CPU | Qwen2.5 correction |
| **ğŸ”Š TTS Latency** | **< 80ms** | CPU | KokoroTTS synthesis |

### ğŸ“ˆ End-to-End Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Video Input  â”‚ Sign Detect  â”‚ Grammar Fix  â”‚ Voice Output â”‚
â”‚   (Real-time)â”‚   (~100ms)   â”‚   (~200ms)   â”‚   (~80ms)    â”‚
â”‚              â”‚              â”‚              â”‚              â”‚
â”‚      ğŸ‘‹      â”‚      ğŸ§       â”‚      ğŸ¤–      â”‚      ğŸ”Š      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         Total Latency: ~380ms (Perceived as Real-time)
```

</div>

**Note**: Confusion matrices and training curves are available in the `results/` directory.

---

## ğŸ“¦ Installation

### ğŸ”§ Prerequisites

```bash
âœ… Python 3.8+
âœ… Webcam (for live inference) or video files
âœ… NVIDIA GPU (optional, for faster training)
âœ… 4GB RAM minimum (8GB recommended)
```

### ğŸš€ Quick Start

```bash
# 1ï¸âƒ£ Clone the repository
git clone https://github.com/Atul013/SIGN2SOUND_ABH.git
cd SIGN2SOUND_ABH

# 2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

# 3ï¸âƒ£ Download pre-trained models (if available)
# Place model weights in checkpoints/
# - best_model.pth (Sign classifier)
# - qwen2.5-1.5b-instruct-gptq (Grammar model)
# - kokoro-v0.19.pth (TTS model)
```

### ğŸ¨ Optional: Install Advanced Features

```bash
# For Qwen2.5 LLM support
pip install transformers accelerate bitsandbytes

# For KokoroTTS support
pip install kokoro-onnx phonemizer
```

---

## ğŸ’» Usage

### ğŸ¬ Demo Modes

<table>
<tr>
<td width="50%">

#### ğŸ”´ Real-time Webcam Demo

```bash
python inference/realtime_demo.py \
  --model checkpoints/best_model.pth \
  --use-grammar \
  --use-tts
```

**Features**:
- âœ… Live sign detection
- âœ… Qwen2.5 grammar correction
- âœ… KokoroTTS voice output
- âœ… Visual feedback overlay

</td>
<td width="50%">

#### ğŸ“¹ Video File Processing

```bash
python inference/infer.py \
  --input data/test_video.mp4 \
  --model checkpoints/best_model.pth \
  --output results/output.txt
```

**Features**:
- âœ… Batch processing
- âœ… Frame-by-frame analysis
- âœ… Export to text/JSON
- âœ… Performance metrics

</td>
</tr>
</table>

### ğŸ› ï¸ Training Pipeline

```bash
# 1ï¸âƒ£ Preprocess raw videos
python preprocessing/preprocess.py \
  --input data/raw_videos \
  --output data/landmarks

# 2ï¸âƒ£ Train the temporal model
python training/train.py \
  --config training/config.yaml \
  --epochs 50 \
  --batch-size 32

# 3ï¸âƒ£ Evaluate on test set
python training/evaluate.py \
  --model checkpoints/best_model.pth \
  --test-data data/test_set
```

---

## ğŸ“‚ Dataset Information

<div align="center">

### ğŸ“š Primary Data Sources

| Dataset | Type | Signs/Classes | Size | Usage |
|:--------|:-----|:-------------:|:----:|:------|
| ï¿½ï¿½ **ASL Alphabet (Kaggle)** | Image Dataset | **29 classes** | **~87,000 images** | **Primary Training** |
| ï¿½ï¿½ **ISL Skeletal** | Pre-extracted landmarks | 50+ | Medium | Validation |
| ğŸ‡®ğŸ‡³ **Malayalam SL** | Regional signs | 20+ | Small | Diversity testing |

</div>

---

### ğŸŒŸ Featured Dataset: ASL Alphabet (Kaggle)

<div align="center">

**ğŸ“¦ [Download from Kaggle â†’](https://www.kaggle.com/datasets/debashishsau/aslamerican-sign-language-aplhabet-dataset)**

[![Kaggle](https://img.shields.io/badge/Kaggle-Dataset-20BEFF?style=for-the-badge&logo=kaggle&logoColor=white)](https://www.kaggle.com/datasets/debashishsau/aslamerican-sign-language-aplhabet-dataset)
[![License](https://img.shields.io/badge/License-CC0%20Public%20Domain-green?style=for-the-badge)](https://creativecommons.org/publicdomain/zero/1.0/)
[![Downloads](https://img.shields.io/badge/Downloads-10.7K+-blue?style=for-the-badge)](https://www.kaggle.com/datasets/debashishsau/aslamerican-sign-language-aplhabet-dataset)

</div>

#### ğŸ“Š Dataset Specifications

<table>
<tr>
<td width="50%">

**ğŸ“ Content Structure**
- âœ… **29 Classes Total**
  - 26 Alphabets (A-Z)
  - 3 Special Classes (SPACE, DELETE, NOTHING)
- âœ… **~87,000 Training Images**
- âœ… **29 Test Images** (encourages real-world testing)
- âœ… **Organized Folder Structure**

</td>
<td width="50%">

**ğŸ¯ Key Features**
- âœ… **High-Quality Images**: Clear hand gestures
- âœ… **Diverse Backgrounds**: Various lighting conditions
- âœ… **Multiple Angles**: Different hand orientations
- âœ… **CC0 License**: Public domain, free to use
- âœ… **Ready for Classification**: Pre-labeled and organized

</td>
</tr>
</table>

#### ğŸ’¡ Why This Dataset?

> **Bridging Communication Gaps**: This dataset was created specifically to reduce the communication barrier between sign language users and non-sign language users. It's perfect for training robust ASL alphabet recognition models!

**Dataset Stats**:
```
ğŸ“¦ Total Size:      ~4.5 GB (compressed)
ğŸ“¸ Image Format:    JPG/PNG
ğŸ¯ Classes:         29 (A-Z + SPACE + DELETE + NOTHING)
ğŸ‘¥ Contributors:    Open-source community
â­ Popularity:      52K+ views, 10.7K+ downloads, 83 upvotes
```

#### ğŸš€ Quick Start with This Dataset

```bash
# 1ï¸âƒ£ Download from Kaggle
# Visit: https://www.kaggle.com/datasets/debashishsau/aslamerican-sign-language-aplhabet-dataset
# Or use Kaggle API:
kaggle datasets download -d debashishsau/aslamerican-sign-language-aplhabet-dataset

# 2ï¸âƒ£ Extract the dataset
unzip aslamerican-sign-language-aplhabet-dataset.zip -d data/raw/

# 3ï¸âƒ£ Preprocess for training
python preprocessing/preprocess.py \
  --input data/raw/asl_alphabet_train \
  --output data/processed/landmarks \
  --extract-landmarks
```

---

### ğŸ“‹ Operational Sign Set

Our current implementation focuses on a **validation subset** across multiple categories:

<div align="center">

| Category | Examples | Count | Temporal Complexity |
|:--------:|:---------|:-----:|:-------------------:|
| ğŸ”¤ **Alphabet** | A-Z fingerspelling | 26 | Static/Low |
| ğŸ‘‹ **Greetings** | Hello, Thank You, Sorry | 5-10 | Medium |
| ğŸ½ï¸ **Needs** | Help, Water, Food | 5-10 | Medium-High |
| âœ… **Responses** | Yes, No, Maybe | 3-5 | Low-Medium |
| ğŸ”¢ **Numbers** | 0-10 | 11 | Low |

**Total Operational Signs**: 30-50 signs (expandable to 100+ in Phase 3)

</div>

### ğŸ“Š Data Splits

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Distribution                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  ğŸ“Š Training:   70% (Augmented with temporal jitter)    â”‚
â”‚     â””â”€ Techniques: Rotation, scaling, temporal shifts   â”‚
â”‚                                                          â”‚
â”‚  ğŸ“Š Validation: 15% (Hyperparameter tuning)             â”‚
â”‚     â””â”€ Used for early stopping and model selection      â”‚
â”‚                                                          â”‚
â”‚  ğŸ“Š Test:       15% (Held-out, person-independent)      â”‚
â”‚     â””â”€ Final evaluation, never seen during training     â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Note**: Splits are stratified by participant (when metadata is available) to ensure person-independent evaluation and prevent overfitting.



---

## ğŸ”® Future Roadmap

<div align="center">

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸš€ Development Phases                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  âœ… Phase 1: Core Pipeline (COMPLETED)                  â”‚
â”‚     â””â”€ MediaPipe + LSTM + Basic TTS                     â”‚
â”‚                                                          â”‚
â”‚  ğŸ”„ Phase 2: Intelligence Upgrade (IN PROGRESS)         â”‚
â”‚     â”œâ”€ âœ… Qwen2.5 Grammar Correction                    â”‚
â”‚     â”œâ”€ âœ… KokoroTTS Integration                         â”‚
â”‚     â””â”€ ğŸ”„ Expand to 100+ signs                          â”‚
â”‚                                                          â”‚
â”‚  ğŸ“‹ Phase 3: Production Ready (PLANNED)                 â”‚
â”‚     â”œâ”€ Mobile deployment (TFLite/ONNX)                  â”‚
â”‚     â”œâ”€ Hardware integration (depth cameras)             â”‚
â”‚     â”œâ”€ Multi-language support                           â”‚
â”‚     â””â”€ Streaming real-time decoder                      â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

### ğŸ¯ Upcoming Features

- [ ] **ğŸŒ Multi-Language**: Support for ISL, ASL, BSL, and regional variants
- [ ] **ğŸ“± Mobile App**: Android/iOS deployment with on-device inference
- [ ] **ğŸ¥ Depth Sensing**: Intel RealSense integration for 3D hand tracking
- [ ] **ğŸ”„ Streaming Mode**: Asynchronous token decoding for lower latency
- [ ] **ğŸ¨ Custom Voices**: Train personalized KokoroTTS voice profiles
- [ ] **ğŸ“Š Analytics Dashboard**: Real-time performance monitoring

---

## ğŸ‘¥ Team

<div align="center">

<table>
<tr>
<td align="center" width="33%">
<img src="https://github.com/identicons/zayed.png" width="100px" style="border-radius:50%"/><br/>
<b>Zayed Bin Hassan</b><br/>
<sub>AI Engineer & Architecture</sub><br/>
<a href="https://github.com/zayed">GitHub</a>
</td>
<td align="center" width="33%">
<img src="https://github.com/identicons/amal.png" width="100px" style="border-radius:50%"/><br/>
<b>Amal Babu</b><br/>
<sub>Data Processing & Model Training</sub><br/>
<a href="https://github.com/amal">GitHub</a>
</td>
<td align="center" width="33%">
<img src="https://github.com/identicons/atul.png" width="100px" style="border-radius:50%"/><br/>
<b>Atul Biju</b><br/>
<sub>Feature Engineering & Deployment</sub><br/>
<a href="https://github.com/Atul013">GitHub</a>
</td>
</tr>
</table>

</div>

---

## ğŸ† Key Technologies

<div align="center">

### ğŸ§  AI/ML Stack

[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![MediaPipe](https://img.shields.io/badge/MediaPipe-00C4CC?style=for-the-badge&logo=google&logoColor=white)](https://mediapipe.dev/)
[![Transformers](https://img.shields.io/badge/ğŸ¤—_Transformers-FFD21E?style=for-the-badge)](https://huggingface.co/transformers)
[![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)](https://numpy.org/)
[![OpenCV](https://img.shields.io/badge/OpenCV-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)](https://opencv.org/)

### ğŸ¯ Specialized Models

<table>
<tr>
<td align="center" width="50%">
<h3>ğŸ¤– Qwen2.5-1.5B-Instruct</h3>
<b>Grammar Correction Engine</b><br/><br/>
âœ… 4-bit GPTQ Quantization<br/>
âœ… Context-aware gloss correction<br/>
âœ… < 200ms latency on CPU<br/>
âœ… Fully offline execution<br/><br/>
<a href="https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct">Model Card â†’</a>
</td>
<td align="center" width="50%">
<h3>ğŸ”Š KokoroTTS v0.19</h3>
<b>Neural Speech Synthesis</b><br/><br/>
âœ… High-fidelity, expressive voices<br/>
âœ… < 80ms synthesis latency<br/>
âœ… Multiple voice profiles<br/>
âœ… Emotional intonation support<br/><br/>
<a href="https://github.com/hexgrad/kokoro">GitHub Repo â†’</a>
</td>
</tr>
</table>

</div>

---

## ğŸ“œ License

<div align="center">

Distributed under the **MIT License**. See [`LICENSE`](LICENSE) for more information.

```
MIT License - Free to use, modify, and distribute
```

</div>

---

## ğŸ™ Acknowledgments

<div align="center">

Special thanks to the open-source community and these amazing projects:

ğŸ™Œ **[MediaPipe](https://mediapipe.dev/)** by Google - Landmark extraction framework  
ğŸ™Œ **[Qwen Team](https://huggingface.co/Qwen)** - Small Language Model for grammar correction  
ğŸ™Œ **[KokoroTTS](https://github.com/hexgrad/kokoro)** - High-fidelity neural TTS engine  
ğŸ™Œ **[PyTorch](https://pytorch.org/)** - Deep learning framework  
ğŸ™Œ **Competition Organizers** - Dataset provision and guidelines  

</div>

---

<div align="center">

### ğŸŒŸ Star this repo if you find it useful!

**Sign2Sound** â€“ *Empowering accessible communication through AI* ğŸš€

[![GitHub stars](https://img.shields.io/github/stars/Atul013/SIGN2SOUND_ABH?style=social)](https://github.com/Atul013/SIGN2SOUND_ABH)
[![GitHub forks](https://img.shields.io/github/forks/Atul013/SIGN2SOUND_ABH?style=social)](https://github.com/Atul013/SIGN2SOUND_ABH/fork)

---

Made with â¤ï¸ by Team ABH

</div>
